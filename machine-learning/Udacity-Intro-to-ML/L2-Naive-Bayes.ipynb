{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Naive Bayes to categorize emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data into the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "with open(\"../data/email_authors.pkl\", 'rb') as authors_file, open(\"../data/word_data.pkl\", 'rb') as word_file:\n",
    "    email_authors = pickle.load(authors_file)\n",
    "    word_data = pickle.load(word_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train, features_test, labels_train, labels_test = train_test_split(word_data, email_authors, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize time: 2.274 s\n"
     ]
    }
   ],
   "source": [
    "# tokenize emails\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "t_tokenize = time()\n",
    "features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "print(\"tokenize time:\", round(time()-t_tokenize, 3), \"s\")\n",
    "features_test_transformed = vectorizer.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selector time: 0.39 s\n"
     ]
    }
   ],
   "source": [
    "# only use top 10% of features\n",
    "selector = SelectPercentile(percentile=10)\n",
    "t_selector = time()\n",
    "features_train_transformed = selector.fit_transform(features_train_transformed, labels_train).toarray()\n",
    "print(\"selector time:\", round(time()-t_selector, 3), \"s\")\n",
    "features_test_transformed = selector.transform(features_test_transformed).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Gaussian Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_GaussianNB fit time: 0.997 s\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "t_GaussianNB_fit = time()\n",
    "gnb.fit(features_train_transformed, labels_train)\n",
    "print(\"t_GaussianNB fit time:\", round(time()-t_GaussianNB_fit, 3), \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t_GaussianNB predict time: 0.132 s\n"
     ]
    }
   ],
   "source": [
    "t_GaussianNB_predict = time()\n",
    "labels_pred = gnb.predict(features_test_transformed)\n",
    "print(\"t_GaussianNB predict time:\", round(time()-t_GaussianNB_predict, 3), \"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1758 points : 47\n",
      "Accuracy of: 0.9732650739476678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\" % (features_test_transformed.shape[0], (labels_test != labels_pred).sum()))\n",
    "print(\"Accuracy of:\", accuracy_score(labels_test, labels_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Working through implementing a text classifier piece by piece allowed me to learn more about the overall process of text based learning.\n",
    "\n",
    "I was surprised at how little resources the NB classifier used.\n",
    "\n",
    "Having a lot of time spent in the tokenizer isn't surprising since there's a lot of text and I suspect the algorithm is greater than linear WRT chars. However, it took a lot more time than I expected. I looked up a couple articles and it looks like the runtime of the tokenizer is a pretty common issue.\n",
    "\n",
    "It surprised me that we threw away 90% of the features and still got such a good result. I'm sure this could be explained by the data being used, however, the idea was so absurd to me that I would never have thought to do it without seeing it first."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
