{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Decision Tree to categorize emails\n",
    "\n",
    "## Decision Tree Mini Project\n",
    "\n",
    "In this project, we will again try to classify emails, this time using a decision tree.   The starter code is in decision_tree/dt_author_id.py.\n",
    "\n",
    "### Part 1: Get the Decision Tree Running\n",
    "Get the decision tree up and running as a classifier, setting min_samples_split=40.  It will probably take a while to train.  What’s the accuracy?\n",
    "\n",
    "### Part 2: Speed It Up\n",
    "You found in the SVM mini-project that the parameter tune can significantly speed up the training time of a machine learning algorithm.  A general rule is that the parameters can tune the complexity of the algorithm, with more complex algorithms generally running more slowly.  \n",
    "\n",
    "Another way to control the complexity of an algorithm is via the number of features that you use in training/testing.  The more features the algorithm has available, the more potential there is for a complex fit.  We will explore this in detail in the “Feature Selection” lesson, but you’ll get a sneak preview now.\n",
    "\n",
    "- find the number of features in your data.  The data is organized into a numpy array where the number of rows is the number of data points and the number of columns is the number of features; so to extract this number, use a line of code like len(features_train[0])\n",
    "- go into tools/email_preprocess.py, and find the line of code that looks like this:     selector = SelectPercentile(f_classif, percentile=1)  Change percentile from 10 to 1.\n",
    "- What’s the number of features now?\n",
    "- What do you think SelectPercentile is doing?  Would a large value for percentile lead to a more complex or less complex decision tree, all other things being equal?\n",
    "- Note the difference in training time depending on the number of features.  \n",
    "- What’s the accuracy when percentile = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from cache_sklearn_model import retrieve_cached_model, save_cached_model\n",
    "from email_preprocess import preprocess_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocess_emails_data(amount_of_training_data = 1.0, percentile_of_features = 10):\n",
    "    features_train, features_test, labels_train, labels_test = preprocess_emails(percentile_of_features)\n",
    "\n",
    "    print(f\"training on subset of features: ({percentile_of_features}%)\")\n",
    "\n",
    "    if amount_of_training_data == 1:\n",
    "        features = features_train\n",
    "        labels = labels_train\n",
    "    else:\n",
    "        features,_,labels,_ = train_test_split(\n",
    "            features_train,\n",
    "            labels_train,\n",
    "            train_size=amount_of_training_data,\n",
    "            random_state=91, \n",
    "        )\n",
    "\n",
    "    data_percentile = round(len(features)/ (len(features_train) + 0.0000001)*100)\n",
    "    print(f\"training on subset of traning data: {len(features)} out of {len(features_train)} ({data_percentile}%)\")\n",
    "\n",
    "    data_desc = f'preprocess_emails_{round(amount_of_training_data*100)}_{percentile_of_features}'\n",
    "\n",
    "    return [data_desc, features, features_test, labels, labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_dt(amount_of_training_data = 1.0, percentile_of_features = 10, **kwargs):\n",
    "\n",
    "    data_desc, features_train, features_test, labels_train, labels_test = get_preprocess_emails_data(amount_of_training_data, percentile_of_features)\n",
    "\n",
    "    clf = DecisionTreeClassifier(random_state = 45, **kwargs)\n",
    "    [is_restored, clf, meta] = retrieve_cached_model(clf, data_desc)\n",
    "    \n",
    "    fit_delta = meta.get(\"fit_time_sec\")\n",
    "    if not is_restored:\n",
    "        t = time()\n",
    "        clf.fit(features_train, labels_train)\n",
    "        fit_delta = round(time()-t, 3)\n",
    "        save_cached_model(clf, data_desc, {\n",
    "            \"fit_time_sec\": fit_delta,\n",
    "        })\n",
    "    print(\"clf fit time:\", fit_delta, \"s\")\n",
    "    \n",
    "    # output predictions\n",
    "    t = time()\n",
    "    labels_pred = clf.predict(features_test)\n",
    "    pred_delta = round(time()-t, 3)\n",
    "    print(\"clf predict time:\", pred_delta, \"s\")\n",
    "    \n",
    "    # measure\n",
    "    n_nodes = clf.tree_.node_count\n",
    "    n_leaves = clf.get_n_leaves()\n",
    "    depth = clf.get_depth()\n",
    "    print(\"n_nodes:\", n_nodes, \"n_leaves:\", n_leaves, \"depth:\", depth)\n",
    "\n",
    "    print(classification_report(labels_pred, labels_test, digits=4))\n",
    "\n",
    "    accuracy = accuracy_score(labels_pred, labels_test)\n",
    "    print(f'| {amount_of_training_data} | {kwargs} | {fit_delta}s | {pred_delta}s | {accuracy} |')\n",
    "\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How long do DTs take to compute?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 1582 out of 15820 (10%)\n",
      "clf fit time: 1.116 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 279 n_leaves: 140 depth: 87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8578    0.9012    0.8789       850\n",
      "           1     0.9029    0.8601    0.8810       908\n",
      "\n",
      "    accuracy                         0.8800      1758\n",
      "   macro avg     0.8803    0.8807    0.8800      1758\n",
      "weighted avg     0.8811    0.8800    0.8800      1758\n",
      "\n",
      "| 0.1 | {} | 1.116s | 0.014s | 0.8799772468714449 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(amount_of_training_data = 0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 3164 out of 15820 (20%)\n",
      "clf fit time: 5.182 s\n",
      "clf predict time: 0.013 s\n",
      "n_nodes: 379 n_leaves: 190 depth: 119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9239    0.9461    0.9348       872\n",
      "           1     0.9457    0.9233    0.9343       886\n",
      "\n",
      "    accuracy                         0.9346      1758\n",
      "   macro avg     0.9348    0.9347    0.9346      1758\n",
      "weighted avg     0.9348    0.9346    0.9346      1758\n",
      "\n",
      "| 0.2 | {} | 5.182s | 0.013s | 0.934584755403868 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(amount_of_training_data = 0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 6328 out of 15820 (40%)\n",
      "clf fit time: 18.78 s\n",
      "clf predict time: 0.011 s\n",
      "n_nodes: 559 n_leaves: 280 depth: 173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9552    0.9574    0.9563       891\n",
      "           1     0.9561    0.9539    0.9550       867\n",
      "\n",
      "    accuracy                         0.9556      1758\n",
      "   macro avg     0.9556    0.9556    0.9556      1758\n",
      "weighted avg     0.9556    0.9556    0.9556      1758\n",
      "\n",
      "| 0.4 | {} | 18.78s | 0.011s | 0.9556313993174061 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(amount_of_training_data = 0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 12656 out of 15820 (80%)\n",
      "clf fit time: 55.202 s\n",
      "clf predict time: 0.012 s\n",
      "n_nodes: 761 n_leaves: 381 depth: 215\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9731    0.9742    0.9737       892\n",
      "           1     0.9734    0.9723    0.9728       866\n",
      "\n",
      "    accuracy                         0.9733      1758\n",
      "   macro avg     0.9733    0.9733    0.9733      1758\n",
      "weighted avg     0.9733    0.9733    0.9733      1758\n",
      "\n",
      "| 0.8 | {} | 55.202s | 0.012s | 0.9732650739476678 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(amount_of_training_data = 0.80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 77.709 s\n",
      "clf predict time: 0.012 s\n",
      "n_nodes: 813 n_leaves: 407 depth: 232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9889    0.9911       897\n",
      "           1     0.9884    0.9930    0.9907       861\n",
      "\n",
      "    accuracy                         0.9909      1758\n",
      "   macro avg     0.9909    0.9909    0.9909      1758\n",
      "weighted avg     0.9909    0.9909    0.9909      1758\n",
      "\n",
      "| 1 | {} | 77.709s | 0.012s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(amount_of_training_data = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_best = \"| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Your First Email DT: Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 78.901 s\n",
      "clf predict time: 0.012 s\n",
      "n_nodes: 703 n_leaves: 352 depth: 232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9787    0.9765    0.9776       895\n",
      "           1     0.9757    0.9780    0.9769       863\n",
      "\n",
      "    accuracy                         0.9772      1758\n",
      "   macro avg     0.9772    0.9773    0.9772      1758\n",
      "weighted avg     0.9773    0.9772    0.9772      1758\n",
      "\n",
      "| 1.0 | {'min_samples_split': 40} | 78.901s | 0.012s | 0.9772468714448237 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_samples_split=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz: Speeding Up Via Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "number of features currently training on: 3785\n"
     ]
    }
   ],
   "source": [
    "print(\"number of features currently training on:\", len(get_preprocess_emails_data()[1][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy Using 1% of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (1%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 4.997 s\n",
      "clf predict time: 0.002 s\n",
      "n_nodes: 679 n_leaves: 340 depth: 190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9552    0.9793    0.9671       871\n",
      "           1     0.9792    0.9549    0.9669       887\n",
      "\n",
      "    accuracy                         0.9670      1758\n",
      "   macro avg     0.9672    0.9671    0.9670      1758\n",
      "weighted avg     0.9673    0.9670    0.9670      1758\n",
      "\n",
      "| 1.0 | {'min_samples_split': 40} | 4.997s | 0.002s | 0.9670079635949943 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_samples_split=40, percentile_of_features=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do different params perform on this dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### criterion\n",
    "I don't think `criterion` will have much of an impact since both `gini` and `entropy` are very similar and only make a difference in 2% of cases.\n",
    "\n",
    "https://datascience.stackexchange.com/questions/10228/when-should-i-use-gini-impurity-as-opposed-to-information-gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 22.557 s\n",
      "clf predict time: 0.013 s\n",
      "n_nodes: 763 n_leaves: 382 depth: 174\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9922    0.9966    0.9944       889\n",
      "           1     0.9965    0.9919    0.9942       869\n",
      "\n",
      "    accuracy                         0.9943      1758\n",
      "   macro avg     0.9943    0.9943    0.9943      1758\n",
      "weighted avg     0.9943    0.9943    0.9943      1758\n",
      "\n",
      "| 1.0 | {'criterion': 'entropy'} | 22.557s | 0.013s | 0.9943117178612059 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(criterion=\"entropy\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0034129692832763903"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.9943117178612059 - 0.9908987485779295"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using entropy only increases it by 0.0034. I'm doubtful that this is significant.\n",
    "However, I'll look at it again once all the other parameters are tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### splitter\n",
    "How does the `splitter=\"random\"` differ from `max_features`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 46.89 s\n",
      "clf predict time: 0.015 s\n",
      "n_nodes: 863 n_leaves: 432 depth: 240\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9899    0.9910    0.9905       892\n",
      "           1     0.9908    0.9896    0.9902       866\n",
      "\n",
      "    accuracy                         0.9903      1758\n",
      "   macro avg     0.9903    0.9903    0.9903      1758\n",
      "weighted avg     0.9903    0.9903    0.9903      1758\n",
      "\n",
      "| 1.0 | {'splitter': 'random'} | 46.89s | 0.015s | 0.9903299203640501 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(splitter=\"random\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to have a shorter training time and lower accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### class_weight\n",
    "This dataset is already class balanced so class_weight shouldn't make a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "class 1: 8801\n",
      "class 2: 8777\n"
     ]
    }
   ],
   "source": [
    "def temp():\n",
    "    data_desc, features_train, features_test, labels_train, labels_test = get_preprocess_emails_data()\n",
    "    all_labels = labels_train + labels_test\n",
    "    print(f'class 1: {sum(all_labels)}')\n",
    "    print(f'class 2: {len(all_labels)-sum(all_labels)}')\n",
    "temp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 80.095 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 813 n_leaves: 407 depth: 232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9889    0.9911       897\n",
      "           1     0.9884    0.9930    0.9907       861\n",
      "\n",
      "    accuracy                         0.9909      1758\n",
      "   macro avg     0.9909    0.9909    0.9909      1758\n",
      "weighted avg     0.9909    0.9909    0.9909      1758\n",
      "\n",
      "| 1.0 | {'class_weight': 'balanced'} | 80.095s | 0.014s | 0.9908987485779295 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(class_weight=\"balanced\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_depth\n",
    "\n",
    "I suspect max_depth will be less effective than `min_samples_split`, `min_samples_leaf`, `min_weight_fraction_leaf`, `max_leaf_nodes`, etc.\n",
    "\n",
    "However, it will allow graphing the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 3.575 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 7 n_leaves: 4 depth: 2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4894    0.9776    0.6522       447\n",
      "           1     0.9884    0.6522    0.7858      1311\n",
      "\n",
      "    accuracy                         0.7349      1758\n",
      "   macro avg     0.7389    0.8149    0.7190      1758\n",
      "weighted avg     0.8615    0.7349    0.7519      1758\n",
      "\n",
      "| 1.0 | {'max_depth': 2} | 3.575s | 0.014s | 0.7349260523321957 |\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plot_tree(optimize_dt(max_depth=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 76.591 s\n",
      "clf predict time: 0.012 s\n",
      "n_nodes: 749 n_leaves: 375 depth: 200\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9922    0.9855    0.9888       899\n",
      "           1     0.9850    0.9919    0.9884       859\n",
      "\n",
      "    accuracy                         0.9886      1758\n",
      "   macro avg     0.9886    0.9887    0.9886      1758\n",
      "weighted avg     0.9886    0.9886    0.9886      1758\n",
      "\n",
      "| 1.0 | {'max_depth': 200} | 76.591s | 0.012s | 0.9886234357224118 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(max_depth=200)\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `min_samples_split`\n",
    "what happens if we double?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 77.916 s\n",
      "clf predict time: 0.019 s\n",
      "n_nodes: 811 n_leaves: 406 depth: 232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9856    0.9894       900\n",
      "           1     0.9850    0.9930    0.9890       858\n",
      "\n",
      "    accuracy                         0.9892      1758\n",
      "   macro avg     0.9891    0.9893    0.9892      1758\n",
      "weighted avg     0.9892    0.9892    0.9892      1758\n",
      "\n",
      "| 1.0 | {'min_samples_split': 4} | 77.916s | 0.019s | 0.9891922639362912 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_samples_split=4)\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 75.32 s\n",
      "clf predict time: 0.013 s\n",
      "n_nodes: 811 n_leaves: 406 depth: 232\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9856    0.9894       900\n",
      "           1     0.9850    0.9930    0.9890       858\n",
      "\n",
      "    accuracy                         0.9892      1758\n",
      "   macro avg     0.9891    0.9893    0.9892      1758\n",
      "weighted avg     0.9892    0.9892    0.9892      1758\n",
      "\n",
      "| 1.0 | {'min_samples_split': 3} | 75.32s | 0.013s | 0.9891922639362912 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_samples_split=3)\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit is you can restrict leaf nodes by a smaller amount. Not sure what the implications of this are in practical terms as you can still have leaf nodes with a single example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `min_samples_leaf`\n",
    "try doubling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 75.351 s\n",
      "clf predict time: 0.012 s\n",
      "n_nodes: 811 n_leaves: 406 depth: 225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9843    0.9821    0.9832       895\n",
      "           1     0.9815    0.9838    0.9826       863\n",
      "\n",
      "    accuracy                         0.9829      1758\n",
      "   macro avg     0.9829    0.9830    0.9829      1758\n",
      "weighted avg     0.9829    0.9829    0.9829      1758\n",
      "\n",
      "| 1.0 | {'min_samples_leaf': 2} | 75.351s | 0.012s | 0.9829351535836177 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_samples_leaf=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `min_weight_fraction_leaf`\n",
    "should be the same as `min_samples_leaf` but weighted, which doesn't apply to this dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `max_leaf_nodes`\n",
    "set to just below existing best: 407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 109.907 s\n",
      "clf predict time: 0.015 s\n",
      "n_nodes: 797 n_leaves: 399 depth: 225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9933    0.9900    0.9916       896\n",
      "           1     0.9896    0.9930    0.9913       862\n",
      "\n",
      "    accuracy                         0.9915      1758\n",
      "   macro avg     0.9914    0.9915    0.9915      1758\n",
      "weighted avg     0.9915    0.9915    0.9915      1758\n",
      "\n",
      "| 1.0 | {'max_leaf_nodes': 399} | 109.907s | 0.015s | 0.9914675767918089 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(max_leaf_nodes=399)\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "max_leaf_nodes seems like the finest grain way to adjust a DT however, using it seems like it would lead to over fitting the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `min_impurity_decrease`\n",
    "I doubt this will help since all the other ways of reducing node complexity has lead to not fitting as well to the training set. At this point I'm concerned that the training and test sets are too similar and that none of these models will generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 1.572 s\n",
      "clf predict time: 0.013 s\n",
      "n_nodes: 1 n_leaves: 1 depth: 0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000         0\n",
      "           1     1.0000    0.4920    0.6596      1758\n",
      "\n",
      "    accuracy                         0.4920      1758\n",
      "   macro avg     0.5000    0.2460    0.3298      1758\n",
      "weighted avg     1.0000    0.4920    0.6596      1758\n",
      "\n",
      "| 1.0 | {'min_impurity_decrease': 0.1} | 1.572s | 0.013s | 0.4920364050056883 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_impurity_decrease=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 74.897 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 595 n_leaves: 298 depth: 136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9854    0.9854       893\n",
      "           1     0.9850    0.9850    0.9850       865\n",
      "\n",
      "    accuracy                         0.9852      1758\n",
      "   macro avg     0.9852    0.9852    0.9852      1758\n",
      "weighted avg     0.9852    0.9852    0.9852      1758\n",
      "\n",
      "| 1.0 | {'min_impurity_decrease': 0.0001} | 74.897s | 0.014s | 0.9852104664391353 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(min_impurity_decrease=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ccp_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def look_at_cost_complexity_pruning_path():\n",
    "#     data_desc, features_train, features_test, labels_train, labels_test = get_preprocess_emails_data()\n",
    "\n",
    "#     clf = optimize_dt(amount_of_training_data = 1)\n",
    "#     path = clf.cost_complexity_pruning_path(features_train, labels_train)\n",
    "#     print(path)\n",
    "# look_at_cost_complexity_pruning_path()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is taking a long time and I'm doubtful that this pruning technique will be any different than the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 73.812 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 609 n_leaves: 305 depth: 138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9899    0.9877    0.9888       895\n",
      "           1     0.9873    0.9896    0.9884       863\n",
      "\n",
      "    accuracy                         0.9886      1758\n",
      "   macro avg     0.9886    0.9886    0.9886      1758\n",
      "weighted avg     0.9886    0.9886    0.9886      1758\n",
      "\n",
      "| 1.0 | {'ccp_alpha': 0.0001} | 73.812s | 0.014s | 0.9886234357224118 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(ccp_alpha=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing Randomness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does max_features do anything when splitter=\"best\"? **yes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 0.925 s\n",
      "clf predict time: 0.013 s\n",
      "n_nodes: 2097 n_leaves: 1049 depth: 278\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9899    0.9910    0.9905       892\n",
      "           1     0.9908    0.9896    0.9902       866\n",
      "\n",
      "    accuracy                         0.9903      1758\n",
      "   macro avg     0.9903    0.9903    0.9903      1758\n",
      "weighted avg     0.9903    0.9903    0.9903      1758\n",
      "\n",
      "| 1.0 | {'max_features': 'sqrt'} | 0.925s | 0.013s | 0.9903299203640501 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(max_features = \"sqrt\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 0.31 s\n",
      "clf predict time: 0.014 s\n",
      "n_nodes: 4945 n_leaves: 2473 depth: 345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9765    0.9831    0.9798       887\n",
      "           1     0.9827    0.9759    0.9793       871\n",
      "\n",
      "    accuracy                         0.9795      1758\n",
      "   macro avg     0.9796    0.9795    0.9795      1758\n",
      "weighted avg     0.9795    0.9795    0.9795      1758\n",
      "\n",
      "| 1.0 | {'max_features': 'log2'} | 0.31s | 0.014s | 0.9795221843003413 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(max_features = \"log2\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (20%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 1.702 s\n",
      "clf predict time: 0.028 s\n",
      "n_nodes: 2801 n_leaves: 1401 depth: 318\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9854    0.9778    0.9816       900\n",
      "           1     0.9769    0.9848    0.9808       858\n",
      "\n",
      "    accuracy                         0.9812      1758\n",
      "   macro avg     0.9812    0.9813    0.9812      1758\n",
      "weighted avg     0.9813    0.9812    0.9812      1758\n",
      "\n",
      "| 1.0 | {'max_features': 'sqrt'} | 1.702s | 0.028s | 0.9812286689419796 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "# if it's so fast can we add more features?\n",
    "_ = optimize_dt(percentile_of_features=20, max_features = \"sqrt\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's probably some tuning in terms of number of features.\n",
    "Figuring out which features expain the data the best is probably something to consider too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (10%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 1.031 s\n",
      "clf predict time: 0.015 s\n",
      "n_nodes: 3697 n_leaves: 1849 depth: 401\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9720    0.9830    0.9775       883\n",
      "           1     0.9827    0.9714    0.9770       875\n",
      "\n",
      "    accuracy                         0.9772      1758\n",
      "   macro avg     0.9773    0.9772    0.9772      1758\n",
      "weighted avg     0.9773    0.9772    0.9772      1758\n",
      "\n",
      "| 1.0 | {'splitter': 'random', 'max_features': 'sqrt'} | 1.031s | 0.015s | 0.9772468714448237 |\n",
      "| 1 | {} | 77.709s | 0.013s | 0.9908987485779295 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(splitter=\"random\", max_features = \"sqrt\")\n",
    "print(current_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize training of DT for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (20%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 138.702 s\n",
      "clf predict time: 0.03 s\n",
      "n_nodes: 821 n_leaves: 411 depth: 234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9955    0.9933    0.9944       895\n",
      "           1     0.9931    0.9954    0.9942       863\n",
      "\n",
      "    accuracy                         0.9943      1758\n",
      "   macro avg     0.9943    0.9943    0.9943      1758\n",
      "weighted avg     0.9943    0.9943    0.9943      1758\n",
      "\n",
      "| 1.0 | {} | 138.702s | 0.03s | 0.9943117178612059 |\n"
     ]
    }
   ],
   "source": [
    "# try increasing the number of features\n",
    "_ = optimize_dt(percentile_of_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on subset of features: (20%)\n",
      "training on subset of traning data: 15820 out of 15820 (100%)\n",
      "clf fit time: 45.27 s\n",
      "clf predict time: 0.03 s\n",
      "n_nodes: 781 n_leaves: 391 depth: 190\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9944    0.9944    0.9944       893\n",
      "           1     0.9942    0.9942    0.9942       865\n",
      "\n",
      "    accuracy                         0.9943      1758\n",
      "   macro avg     0.9943    0.9943    0.9943      1758\n",
      "weighted avg     0.9943    0.9943    0.9943      1758\n",
      "\n",
      "| 1.0 | {'criterion': 'entropy'} | 45.27s | 0.03s | 0.9943117178612059 |\n"
     ]
    }
   ],
   "source": [
    "_ = optimize_dt(percentile_of_features=20, criterion=\"entropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Using more data did increased the quality of the outcomes.\n",
    "Using entropy instead of the gini index did seems to result in the best solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "> Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree.\n",
    "\n",
    "How do you balance a data by class?\n",
    "**Using `class_weight=\"balanced\"`**\n",
    "\n",
    "> There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems.\n",
    "\n",
    "What does it mean to have an XOR, parity, or multiplexer problems in data?\n",
    "\n",
    "**XOR**: when one feature, or the other exist there is a positive effect, if they both exist it is negative. This should be true for Naive Bayes since each feature is assessed independently.\n",
    "\n",
    "> Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble.\n",
    "\n",
    "What is an ensemble and how do you use it?\n",
    "**Should be in the next section**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
